{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"icsx-ctu-extended\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from flow_features import *\n",
    "from flow_analysis import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(flows_path):\n",
    "    # TODO: extract malicious flows metadata by reading directory names in flows_path\n",
    "     \n",
    "    # Read data\n",
    "    benign_df = pd.read_parquet(f'{flows_path}/benign')\n",
    "    malicious_df = pd.read_parquet(f'{flows_path}/malicious')\n",
    "\n",
    "    # Label the data\n",
    "    benign_df['label'] = 0  # BENIGN\n",
    "    malicious_df['label'] = 1  # MALICIOUS\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([benign_df, malicious_df], ignore_index=True)\n",
    "\n",
    "    # Filter out flows where packets_count is less than 3\n",
    "    combined_df = combined_df[combined_df['packets_count'] >= 3]\n",
    "\n",
    "    # Separate features and labels\n",
    "    labels = combined_df['label'].values\n",
    "    features_df = combined_df.drop(['label'], axis=1)\n",
    "\n",
    "    # Convert DataFrame to numpy array using flows_df_to_np\n",
    "    features, metas = flows_df_to_np(features_df)\n",
    "    \n",
    "    return features, labels, metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for dataset: icsx-ctu-extended\n",
      "Train features shape: (289564, 36)\n",
      "Test features shape: (623300, 36)\n",
      "Training:\n",
      "    # malicious flows: 150386 (51.94%)\n",
      "    # benign flows: 139178 (48.06%)\n",
      "Testing:\n",
      "    # malicious flows: 576823 (92.54%)\n",
      "    # benign flows: 46477 (7.46%)\n"
     ]
    }
   ],
   "source": [
    "print (f\"Preparing data for dataset: {dataset}\")\n",
    "\n",
    "# Prepare data\n",
    "train_features, train_labels, train_meta = prepare_data(f'./../flows/train/{dataset}')\n",
    "test_features, test_labels, test_meta = prepare_data(f'./../flows/test/{dataset}')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "train_malicious_count = len(train_labels[train_labels == 1])\n",
    "train_benign_count = len(train_labels[train_labels == 0])\n",
    "test_malicious_count = len(test_labels[test_labels == 1])\n",
    "test_benign_count = len(test_labels[test_labels == 0])\n",
    "\n",
    "# Print number of malicious flows in train and test sets\n",
    "print(\"Training:\")\n",
    "print(f\"    # malicious flows: {train_malicious_count} ({train_malicious_count / len(train_labels) * 100:.2f}%)\")\n",
    "print(f\"    # benign flows: {train_benign_count} ({train_benign_count / len(train_labels) * 100:.2f}%)\")\n",
    "\n",
    "print(\"Testing:\")\n",
    "print(f\"    # malicious flows: {test_malicious_count} ({test_malicious_count / len(test_labels) * 100:.2f}%)\")\n",
    "print(f\"    # benign flows: {test_benign_count} ({test_benign_count / len(test_labels) * 100:.2f}%)\")\n",
    "\n",
    "# Fit Min-Max scaling\n",
    "scaler = MinMaxScaler(feature_range=(0,1)).fit(train_features)\n",
    "\n",
    "with open(f'./../artifacts/{dataset}/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "pca_path = f'./../artifacts/{dataset}/pca_12.pkl'\n",
    "\n",
    "# Load the PCA from file\n",
    "with open(pca_path, 'rb') as f:\n",
    "    pca = pickle.load(f)\n",
    "\n",
    "pca_12 = PCA(n_components=12).fit(train_features)\n",
    "os.makedirs(f'./../artifacts/{dataset}', exist_ok=True)\n",
    "with open(f'./../artifacts/{dataset}/pca_12.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_12, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn16 = tf.keras.models.load_model(f'./../artifacts/{dataset}/dnn_16_16_16.keras')\n",
    "dnn24 = tf.keras.models.load_model(f'./../artifacts/{dataset}/dnn_24_24_24.keras')\n",
    "rf9 = pickle.load(open(f'./../artifacts/{dataset}/rf_9.pkl', 'rb'))\n",
    "pca12_rf9 = pickle.load(open(f'./../artifacts/{dataset}/pca_12_rf_9.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for scaling: 0.05 seconds\n",
      "Time spent for PCA transformation: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "print(f\"Time spent for scaling: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "train_features_pca_12 = pca_12.transform(train_features)\n",
    "test_features_pca_12 = pca_12.transform(test_features)\n",
    "print(f\"Time spent for PCA transformation: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for DNN 16x16x16 (test): 5.01 seconds\n",
      "Items predicted per second: 124427.51\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "predictions16 = dnn16.predict(test_features, verbose=0)\n",
    "time_spent16 = time.time() - start_time\n",
    "print(f\"Time for DNN 16x16x16 (test): {time_spent16:.2f} seconds\")\n",
    "items_per_second = len(predictions16) / time_spent16\n",
    "print(f\"Items predicted per second: {items_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for DNN 16x16x16 (test): 296.16 seconds\n",
      "Items predicted per second: 2104.60\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "predictions16 = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(test_features), batch_size):\n",
    "    batch = test_features[i:i+batch_size]\n",
    "    batch_pred = dnn16.predict(batch, verbose=0)\n",
    "    predictions16.append(batch_pred)\n",
    "\n",
    "# Concatenate all batches into a single array\n",
    "predictions16 = np.concatenate(predictions16, axis=0)\n",
    "\n",
    "time_spent16 = time.time() - start_time\n",
    "print(f\"Time for DNN 16x16x16 (test): {time_spent16:.2f} seconds\")\n",
    "items_per_second = len(predictions16) / time_spent16\n",
    "print(f\"Items predicted per second: {items_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for DNN 24x24x24 (test): 5.059830904006958 seconds\n",
      "Items predicted per second: 123185.93\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "predictions24 = dnn24.predict(test_features, verbose=0)\n",
    "time_spent24 = time.time() - start_time\n",
    "print(f\"Time for DNN 24x24x24 (test): {time_spent24} seconds\")\n",
    "items_per_second = len(predictions24) / time_spent24\n",
    "print(f\"Items predicted per second: {items_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for DNN 24x24x24 (test): 300.215380191803 seconds\n",
      "Items predicted per second: 2076.18\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "predictions24 = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(test_features), batch_size):\n",
    "    batch = test_features[i:i+batch_size]\n",
    "    batch_pred = dnn24.predict(batch, verbose=0)\n",
    "    predictions24.append(batch_pred)\n",
    "\n",
    "# Concatenate all batches into a single array\n",
    "predictions24 = np.concatenate(predictions24, axis=0)\n",
    "\n",
    "time_spent24 = time.time() - start_time\n",
    "print(f\"Time for DNN 24x24x24 (test): {time_spent24} seconds\")\n",
    "items_per_second = len(predictions24) / time_spent24\n",
    "print(f\"Items predicted per second: {items_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for RF_9 (test): 0.77 seconds\n",
      "Items predicted per second: 805748.21\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rf9_predictions = rf9.predict(test_features)\n",
    "time_spent_rf9 = time.time() - start_time\n",
    "print(f\"Time for RF_9 (test): {time_spent_rf9:.2f} seconds\")\n",
    "items_per_second = len(rf9_predictions) / time_spent_rf9\n",
    "print(f\"Items predicted per second: {items_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for RF_9 (test): 14.72 seconds\n",
      "Items predicted per second: 42344.27\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "rf9_predictions = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(test_features), batch_size):\n",
    "    batch = test_features[i:i+batch_size]\n",
    "    batch_pred = rf9.predict(batch)\n",
    "    rf9_predictions.append(batch_pred)\n",
    "\n",
    "# Concatenate all batches into a single array\n",
    "rf9_predictions = np.concatenate(rf9_predictions, axis=0)\n",
    "\n",
    "time_spent_rf9 = time.time() - start_time\n",
    "print(f\"Time for RF_9 (test): {time_spent_rf9:.2f} seconds\")\n",
    "items_per_second = len(rf9_predictions) / time_spent_rf9\n",
    "print(f\"Items predicted per second: {items_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for PCA_12 + RF_9 (test): 0.54 seconds\n",
      "Items predicted per second: 1144971.87\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pca12_rf9_predictions = pca12_rf9.predict(test_features_pca_12)\n",
    "time_spent_pca12_rf9 = time.time() - start_time\n",
    "print(f\"Time for PCA_12 + RF_9 (test): {time_spent_pca12_rf9:.2f} seconds\")\n",
    "items_per_second = len(pca12_rf9_predictions) / time_spent_pca12_rf9\n",
    "print(f\"Items predicted per second: {items_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for PCA_12 + RF_9 (test): 14.18 seconds\n",
      "Items predicted per second: 43970.61\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "pca12_rf9_predictions = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(test_features_pca_12), batch_size):\n",
    "    batch = test_features_pca_12[i:i+batch_size]\n",
    "    batch_pred = pca12_rf9.predict(batch)\n",
    "    pca12_rf9_predictions.append(batch_pred)\n",
    "\n",
    "# Concatenate all batches into a single array\n",
    "pca12_rf9_predictions = np.concatenate(pca12_rf9_predictions, axis=0)\n",
    "\n",
    "time_spent_pca12_rf9 = time.time() - start_time\n",
    "print(f\"Time for PCA_12 + RF_9 (test): {time_spent_pca12_rf9:.2f} seconds\")\n",
    "items_per_second = len(pca12_rf9_predictions) / time_spent_pca12_rf9\n",
    "print(f\"Items predicted per second: {items_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN (24x24x24): AUROC 0.97511784624462\n",
      "DNN (16x16x16): AUROC 0.9713893276719773\n"
     ]
    }
   ],
   "source": [
    "print(\"DNN (24x24x24): AUROC \" + str(roc_auc_score(test_labels, predictions24)))\n",
    "print(\"DNN (16x16x16): AUROC \" + str(roc_auc_score(test_labels, predictions16)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
